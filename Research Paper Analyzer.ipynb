{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HT0I0CngCrl-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hJDy27A2Cx8v",
        "outputId": "be2f0bde-826d-428a-a9f6-5294ee5cd328"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MSmDbns8Z7n",
        "outputId": "0d9da25e-ab51-46c7-ea28-82994982516f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pdf2image) (9.4.0)\n",
            "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pdf2image\n",
            "Successfully installed pdf2image-1.17.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pdf2image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_TbwwDF8hOL",
        "outputId": "8b2835d3-7080-4d86-8152-43ad26b11849"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.1)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (9.4.0)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n"
          ]
        }
      ],
      "source": [
        "!pip install pytesseract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6We1KQG8ymp",
        "outputId": "05cf323d-ebb0-48a6-d5d6-ffa43b200b94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.5 [186 kB]\n",
            "Fetched 186 kB in 2s (121 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 123595 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.5_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.5) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.5) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "!apt-get install poppler-utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPBanM2d8_fa",
        "outputId": "d25b2522-fd32-4815-c75c-010db8d4cc59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  tesseract-ocr-eng tesseract-ocr-osd\n",
            "The following NEW packages will be installed:\n",
            "  tesseract-ocr tesseract-ocr-eng tesseract-ocr-osd\n",
            "0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 4,816 kB of archives.\n",
            "After this operation, 15.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-eng all 1:4.00~git30-7274cfa-1.1 [1,591 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr-osd all 1:4.00~git30-7274cfa-1.1 [2,990 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tesseract-ocr amd64 4.1.1-2.1build1 [236 kB]\n",
            "Fetched 4,816 kB in 2s (1,943 kB/s)\n",
            "Selecting previously unselected package tesseract-ocr-eng.\n",
            "(Reading database ... 123625 files and directories currently installed.)\n",
            "Preparing to unpack .../tesseract-ocr-eng_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr-osd.\n",
            "Preparing to unpack .../tesseract-ocr-osd_1%3a4.00~git30-7274cfa-1.1_all.deb ...\n",
            "Unpacking tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Selecting previously unselected package tesseract-ocr.\n",
            "Preparing to unpack .../tesseract-ocr_4.1.1-2.1build1_amd64.deb ...\n",
            "Unpacking tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Setting up tesseract-ocr-eng (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr-osd (1:4.00~git30-7274cfa-1.1) ...\n",
            "Setting up tesseract-ocr (4.1.1-2.1build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "!apt-get install tesseract-ocr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0WIbRxPvis-"
      },
      "outputs": [],
      "source": [
        "# prompt: delete the folder /content/analysis_results\n",
        "\n",
        "!rm -r /content/extracted_images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iyHoBvLrJ47",
        "outputId": "dbffe365-b3a8-41e5-c420-2eb7ffa0b15f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading PyMuPDF-1.24.9-cp310-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting PyMuPDFb==1.24.9 (from pymupdf)\n",
            "  Downloading PyMuPDFb-1.24.9-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.4 kB)\n",
            "Downloading PyMuPDF-1.24.9-cp310-none-manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyMuPDFb-1.24.9-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (15.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, pymupdf\n",
            "Successfully installed PyMuPDFb-1.24.9 pymupdf-1.24.9\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install pymupdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbighvLBHSwl",
        "outputId": "a603e7c3-e877-4106-c78b-f3da36e3c7ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting camelot-py[cv2]\n",
            "  Downloading camelot_py-0.11.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "\u001b[33mWARNING: camelot-py 0.11.0 does not provide the extra 'cv2'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: chardet>=3.0.4 in /usr/local/lib/python3.10/dist-packages (from camelot-py[cv2]) (5.2.0)\n",
            "Requirement already satisfied: click>=6.7 in /usr/local/lib/python3.10/dist-packages (from camelot-py[cv2]) (8.1.7)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from camelot-py[cv2]) (1.26.4)\n",
            "Requirement already satisfied: openpyxl>=2.5.8 in /usr/local/lib/python3.10/dist-packages (from camelot-py[cv2]) (3.1.5)\n",
            "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.10/dist-packages (from camelot-py[cv2]) (2.1.4)\n",
            "Collecting pdfminer.six>=20200726 (from camelot-py[cv2])\n",
            "  Downloading pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting pypdf>=3.0.0 (from camelot-py[cv2])\n",
            "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from camelot-py[cv2]) (0.9.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl>=2.5.8->camelot-py[cv2]) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->camelot-py[cv2]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->camelot-py[cv2]) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->camelot-py[cv2]) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six>=20200726->camelot-py[cv2]) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six>=20200726->camelot-py[cv2]) (43.0.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf>=3.0.0->camelot-py[cv2]) (4.12.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six>=20200726->camelot-py[cv2]) (1.17.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=0.23.4->camelot-py[cv2]) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six>=20200726->camelot-py[cv2]) (2.22)\n",
            "Downloading pdfminer.six-20240706-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading camelot_py-0.11.0-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf, pdfminer.six, camelot-py\n",
            "Successfully installed camelot-py-0.11.0 pdfminer.six-20240706 pypdf-4.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install camelot-py[cv2]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V22N8iDWJi1X",
        "outputId": "5b8bb8b2-6032-49e6-f6ac-f9457a4a3eb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  fonts-droid-fallback fonts-noto-mono fonts-urw-base35 libgs9 libgs9-common libidn12 libijs-0.35\n",
            "  libjbig2dec0 poppler-data\n",
            "Suggested packages:\n",
            "  fonts-noto fonts-freefont-otf | fonts-freefont-ttf fonts-texgyre ghostscript-x\n",
            "  fonts-japanese-mincho | fonts-ipafont-mincho fonts-japanese-gothic | fonts-ipafont-gothic\n",
            "  fonts-arphic-ukai fonts-arphic-uming fonts-nanum\n",
            "The following NEW packages will be installed:\n",
            "  fonts-droid-fallback fonts-noto-mono fonts-urw-base35 ghostscript libgs9 libgs9-common libidn12\n",
            "  libijs-0.35 libjbig2dec0 poppler-data\n",
            "0 upgraded, 10 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 16.7 MB of archives.\n",
            "After this operation, 63.0 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-droid-fallback all 1:6.0.1r16-1.1build1 [1,805 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 poppler-data all 0.4.11-1 [2,171 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-noto-mono all 20201225-1build1 [397 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-urw-base35 all 20200910-1 [6,367 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgs9-common all 9.55.0~dfsg1-0ubuntu5.9 [752 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libidn12 amd64 1.38-4ubuntu1 [60.0 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libijs-0.35 amd64 0.35-15build2 [16.5 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjbig2dec0 amd64 0.19-3build2 [64.7 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgs9 amd64 9.55.0~dfsg1-0ubuntu5.9 [5,033 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ghostscript amd64 9.55.0~dfsg1-0ubuntu5.9 [49.5 kB]\n",
            "Fetched 16.7 MB in 4s (4,533 kB/s)\n",
            "Selecting previously unselected package fonts-droid-fallback.\n",
            "(Reading database ... 123672 files and directories currently installed.)\n",
            "Preparing to unpack .../0-fonts-droid-fallback_1%3a6.0.1r16-1.1build1_all.deb ...\n",
            "Unpacking fonts-droid-fallback (1:6.0.1r16-1.1build1) ...\n",
            "Selecting previously unselected package poppler-data.\n",
            "Preparing to unpack .../1-poppler-data_0.4.11-1_all.deb ...\n",
            "Unpacking poppler-data (0.4.11-1) ...\n",
            "Selecting previously unselected package fonts-noto-mono.\n",
            "Preparing to unpack .../2-fonts-noto-mono_20201225-1build1_all.deb ...\n",
            "Unpacking fonts-noto-mono (20201225-1build1) ...\n",
            "Selecting previously unselected package fonts-urw-base35.\n",
            "Preparing to unpack .../3-fonts-urw-base35_20200910-1_all.deb ...\n",
            "Unpacking fonts-urw-base35 (20200910-1) ...\n",
            "Selecting previously unselected package libgs9-common.\n",
            "Preparing to unpack .../4-libgs9-common_9.55.0~dfsg1-0ubuntu5.9_all.deb ...\n",
            "Unpacking libgs9-common (9.55.0~dfsg1-0ubuntu5.9) ...\n",
            "Selecting previously unselected package libidn12:amd64.\n",
            "Preparing to unpack .../5-libidn12_1.38-4ubuntu1_amd64.deb ...\n",
            "Unpacking libidn12:amd64 (1.38-4ubuntu1) ...\n",
            "Selecting previously unselected package libijs-0.35:amd64.\n",
            "Preparing to unpack .../6-libijs-0.35_0.35-15build2_amd64.deb ...\n",
            "Unpacking libijs-0.35:amd64 (0.35-15build2) ...\n",
            "Selecting previously unselected package libjbig2dec0:amd64.\n",
            "Preparing to unpack .../7-libjbig2dec0_0.19-3build2_amd64.deb ...\n",
            "Unpacking libjbig2dec0:amd64 (0.19-3build2) ...\n",
            "Selecting previously unselected package libgs9:amd64.\n",
            "Preparing to unpack .../8-libgs9_9.55.0~dfsg1-0ubuntu5.9_amd64.deb ...\n",
            "Unpacking libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.9) ...\n",
            "Selecting previously unselected package ghostscript.\n",
            "Preparing to unpack .../9-ghostscript_9.55.0~dfsg1-0ubuntu5.9_amd64.deb ...\n",
            "Unpacking ghostscript (9.55.0~dfsg1-0ubuntu5.9) ...\n",
            "Setting up fonts-noto-mono (20201225-1build1) ...\n",
            "Setting up libijs-0.35:amd64 (0.35-15build2) ...\n",
            "Setting up fonts-urw-base35 (20200910-1) ...\n",
            "Setting up poppler-data (0.4.11-1) ...\n",
            "Setting up libjbig2dec0:amd64 (0.19-3build2) ...\n",
            "Setting up libidn12:amd64 (1.38-4ubuntu1) ...\n",
            "Setting up fonts-droid-fallback (1:6.0.1r16-1.1build1) ...\n",
            "Setting up libgs9-common (9.55.0~dfsg1-0ubuntu5.9) ...\n",
            "Setting up libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.9) ...\n",
            "Setting up ghostscript (9.55.0~dfsg1-0ubuntu5.9) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ],
      "source": [
        "!apt-get install ghostscript"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaYYyeQVJrX7",
        "outputId": "3c15cebc-a18a-4d1a-ee93-a8750853f927"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ghostscript\n",
            "  Downloading ghostscript-0.7-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: setuptools>=38.6.0 in /usr/local/lib/python3.10/dist-packages (from ghostscript) (71.0.4)\n",
            "Downloading ghostscript-0.7-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: ghostscript\n",
            "Successfully installed ghostscript-0.7\n"
          ]
        }
      ],
      "source": [
        "!pip install ghostscript"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V2gELgurHXhY",
        "outputId": "ffc5255b-5792-4b6b-e605-cb70fbb6630f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Table 1 saved as extracted_tables/table_1.csv\n",
            "Table 2 saved as extracted_tables/table_2.csv\n",
            "Table 3 saved as extracted_tables/table_3.csv\n",
            "Table 4 saved as extracted_tables/table_4.csv\n",
            "Table 5 saved as extracted_tables/table_5.csv\n",
            "Table 6 saved as extracted_tables/table_6.csv\n",
            "\n",
            "Table 1:\n",
            "   0 1\n",
            "0    \n",
            "1    \n",
            "2    \n",
            "3    \n",
            "4    \n",
            "5    \n",
            "\n",
            "Table 2:\n",
            "   0 1\n",
            "0    \n",
            "\n",
            "Table 3:\n",
            "                         0       1       2       3       4       5       6  \\\n",
            "0                  Models      RF     SVM      LR     kNN      DT     MLP   \n",
            "1               Mediapipe  0.7942  0.7175  0.7277  0.4053  0.6512  0.6121   \n",
            "2                ResNet50  0.8503  0.8512  0.8594  0.7941  0.7552  0.8577   \n",
            "3                  3D CNN  0.6854  0.6845  0.6539  0.4089  0.6071  0.6606   \n",
            "4  Blepharospasm Detector  0.9684  0.9528  0.9637  0.9628  0.9138  0.9583   \n",
            "\n",
            "         7  \n",
            "0  XGBoost  \n",
            "1   0.7219  \n",
            "2   0.8235  \n",
            "3   0.6518  \n",
            "4   0.9573  \n",
            "\n",
            "Table 4:\n",
            "                                      0                      1             2  \\\n",
            "0                    Mediapipe (80.55)                         Predicted No   \n",
            "1                                       Actual No\\nActual Yes         15\\n6   \n",
            "2                     ResNet50 (88.88)                         Predicted No   \n",
            "3                                       Actual No\\nActual Yes         15\\n3   \n",
            "4                       3D CNN (69.44)                         Predicted No   \n",
            "5                                       Actual No\\nActual Yes         11\\n6   \n",
            "6  Unsupervised Classification (86.11)                         Predicted No   \n",
            "7                                       Actual No\\nActual Yes         13\\n2   \n",
            "8       Blepharospasm Detector (94.44)                         Predicted No   \n",
            "9                                       Actual No\\nActual Yes         14\\n0   \n",
            "\n",
            "               3  \n",
            "0  Predicted Yes  \n",
            "1          1\\n14  \n",
            "2  Predicted Yes  \n",
            "3          1\\n17  \n",
            "4  Predicted Yes  \n",
            "5          5\\n14  \n",
            "6  Predicted Yes  \n",
            "7          3\\n18  \n",
            "8  Predicted Yes  \n",
            "9          2\\n20  \n",
            "\n",
            "Table 5:\n",
            "                                          0                      1  \\\n",
            "0                             Full (94.44)                          \n",
            "1                                           Actual No\\nActual Yes   \n",
            "2                 Eye Closure Only (77.77)                          \n",
            "3                                           Actual No\\nActual Yes   \n",
            "4                  Long Blink Only (80.55)                          \n",
            "5                                           Actual No\\nActual Yes   \n",
            "6                          BN Only (72.22)                          \n",
            "7                                           Actual No\\nActual Yes   \n",
            "8   Eye Closure + Long Blink Ratio (86.11)                          \n",
            "9                                           Actual No\\nActual Yes   \n",
            "10                Eye Closure + BN (77.77)                          \n",
            "11                                          Actual No\\nActual Yes   \n",
            "12           Long Blink Ratio + BN (69.44)                          \n",
            "13                                          Actual No\\nActual Yes   \n",
            "\n",
            "               2              3  \n",
            "0   Predicted No  Predicted Yes  \n",
            "1          14\\n0          2\\n20  \n",
            "2   Predicted No  Predicted Yes  \n",
            "3          12\\n4          4\\n16  \n",
            "4   Predicted No  Predicted Yes  \n",
            "5          11\\n2          5\\n18  \n",
            "6   Predicted No  Predicted Yes  \n",
            "7          13\\n7          3\\n13  \n",
            "8   Predicted No  Predicted Yes  \n",
            "9          14\\n3          2\\n17  \n",
            "10  Predicted No  Predicted Yes  \n",
            "11         13\\n5          3\\n15  \n",
            "12  Predicted No  Predicted Yes  \n",
            "13         13\\n8          3\\n12  \n",
            "\n",
            "Table 6:\n",
            "                                 0       1       2       3       4       5  \\\n",
            "0                        Features      RF     SVM      LR     kNN      DT   \n",
            "1                            Full  0.9684  0.9528  0.9637  0.9628  0.9138   \n",
            "2                Eye Closure Only  0.8582  0.9025  0.8935  0.8666  0.8518   \n",
            "3                 Long Blink Only   0.790  0.8215  0.8198  0.8205  0.7939   \n",
            "4                         BN Only  0.8628  0.8519  0.8454  0.8592  0.8563   \n",
            "5  Eye Closure + Long Blink Ratio  0.8852  0.9159  0.9083  0.9029  0.8721   \n",
            "6                Eye Closure + BN  0.9046  0.9140  0.9008  0.9142  0.8515   \n",
            "7           Long Blink Ratio + BN  0.8797  0.8725  0.8705  0.8741  0.8424   \n",
            "\n",
            "        6        7  \n",
            "0     MLP  XGBoost  \n",
            "1  0.9583   0.9573  \n",
            "2  0.8924   0.8676  \n",
            "3  0.8206   0.8389  \n",
            "4  0.8491   0.8581  \n",
            "5  0.9166   0.8851  \n",
            "6  0.9103   0.8988  \n",
            "7  0.8714   0.8805  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "import camelot\n",
        "\n",
        "# Function to extract tables from a PDF\n",
        "def extract_tables_from_pdf(pdf_path):\n",
        "    # Extract tables from all pages of the PDF\n",
        "    tables = camelot.read_pdf(pdf_path, pages='all')\n",
        "    return tables\n",
        "\n",
        "# Function to save tables as CSV files\n",
        "def save_tables_as_csv(tables, output_folder=\"extracted_tables\"):\n",
        "    import os\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    for i, table in enumerate(tables):\n",
        "        csv_path = os.path.join(output_folder, f'table_{i + 1}.csv')\n",
        "        table.to_csv(csv_path)\n",
        "        print(f\"Table {i + 1} saved as {csv_path}\")\n",
        "\n",
        "# Function to display tables as Pandas DataFrames (optional)\n",
        "def display_tables(tables):\n",
        "    for i, table in enumerate(tables):\n",
        "        print(f\"\\nTable {i + 1}:\\n\", table.df)\n",
        "\n",
        "# Combine everything into a main function\n",
        "def process_pdf_tables(pdf_path, output_folder=\"extracted_tables\", display=False):\n",
        "    # Step 1: Extract tables from the PDF\n",
        "    tables = extract_tables_from_pdf(pdf_path)\n",
        "\n",
        "    # Step 2: Save extracted tables as CSV files\n",
        "    save_tables_as_csv(tables, output_folder)\n",
        "\n",
        "    # Optional: Step 3: Display tables as Pandas DataFrames\n",
        "    if display:\n",
        "        display_tables(tables)\n",
        "\n",
        "# Test the script\n",
        "pdf_path = \"/content/Bleph_ipmi__Copy_ (2).pdf\"\n",
        "process_pdf_tables(pdf_path, display=True)  # Set display=False if you don't want to print the tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4JlaNA8PUhj",
        "outputId": "ffe21328-fe87-4b55-e1c5-ca178cad87df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: google-generativeai in /usr/local/lib/python3.10/dist-packages (0.7.2)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.6.6 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (0.6.6)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.19.1)\n",
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.137.0)\n",
            "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.27.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (3.20.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (2.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google-generativeai) (4.12.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.6.6->google-generativeai) (1.24.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (1.64.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google-generativeai) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic->google-generativeai) (2.20.1)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.6->google-generativeai) (1.48.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.1.4)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2024.7.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GU4LMnePX6s",
        "outputId": "586a6769-984b-44de-d0e6-eec0897a42f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfjTg6ofP9Z0"
      },
      "source": [
        "##  Implement parallel processing for text extraction, image analysis, and table extraction to reduce the overall time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "2iAvTlXot0Eh",
        "outputId": "b2fe804c-d713-47e9-dc26-e1d313d48e0d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:tornado.access:500 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1904.17ms\n",
            "ERROR:tornado.access:500 POST /v1beta/models/gemini-1.5-pro-latest:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 34146.87ms\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import spacy\n",
        "import fitz  # PyMuPDF\n",
        "import camelot\n",
        "from pdf2image import convert_from_path\n",
        "from pytesseract import image_to_string\n",
        "from PIL import Image\n",
        "import google.generativeai as genai\n",
        "from dotenv import load_dotenv\n",
        "import io\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# Load environment variables\n",
        "os.environ['GENAI_API_KEY'] = 'Your API KEY'\n",
        "genai.configure(api_key=os.environ['GENAI_API_KEY'])\n",
        "\n",
        "# Load environment variables from a .env file\n",
        "load_dotenv()\n",
        "genai.configure(api_key=os.getenv('GENAI_API_KEY'))\n",
        "\n",
        "# Load SpaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "class ResearchPaperAnalyzer:\n",
        "    def __init__(self, model_name: str):\n",
        "        self.model = genai.GenerativeModel(model_name=model_name)\n",
        "        self.image_descriptions = {}\n",
        "        self.tables_data = {}\n",
        "        self.extracted_text = \"\"\n",
        "\n",
        "    def extract_text(self, pdf_path):\n",
        "        pages = convert_from_path(pdf_path)\n",
        "        text = \"\"\n",
        "        for page in pages:\n",
        "            text += image_to_string(page)\n",
        "        return text\n",
        "\n",
        "    def segment_text(self, text):\n",
        "        heading_pattern = r\"^\\d+\\.\\s.+\"\n",
        "        paragraphs = text.split(\"\\n\\n\")\n",
        "        sections = []\n",
        "        for para in paragraphs:\n",
        "            para = para.strip()\n",
        "            if re.match(heading_pattern, para):\n",
        "                sections.append({\"type\": \"heading\", \"content\": para})\n",
        "            else:\n",
        "                sections.append({\"type\": \"paragraph\", \"content\": para})\n",
        "        return sections\n",
        "\n",
        "    def rebuild_structure(self, sections):\n",
        "        formatted_text = \"\"\n",
        "        for section in sections:\n",
        "            if section['type'] == 'heading':\n",
        "                formatted_text += f\"\\n\\n### {section['content']} ###\\n\\n\"\n",
        "            else:\n",
        "                formatted_text += f\"\\n{section['content']}\\n\"\n",
        "        return formatted_text\n",
        "\n",
        "    def analyze_text_with_nlp(self, text):\n",
        "        doc = nlp(text)\n",
        "        tokens = [token.text for token in doc]\n",
        "        pos_tags = [(token.text, token.pos_) for token in doc]\n",
        "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "        return tokens, pos_tags, entities\n",
        "\n",
        "    def process_text(self, text):\n",
        "        sections = self.segment_text(text)\n",
        "        structured_text = self.rebuild_structure(sections)\n",
        "\n",
        "        analyzed_data = []\n",
        "        for section in sections:\n",
        "            tokens, pos_tags, entities = self.analyze_text_with_nlp(section['content'])\n",
        "            analyzed_data.append({\n",
        "                \"section_type\": section['type'],\n",
        "                \"content\": section['content'],\n",
        "                \"tokens\": tokens,\n",
        "                \"pos_tags\": pos_tags,\n",
        "                \"entities\": entities\n",
        "            })\n",
        "\n",
        "        return structured_text, analyzed_data\n",
        "\n",
        "    def extract_images(self, pdf_path):\n",
        "        doc = fitz.open(pdf_path)\n",
        "        images = []\n",
        "\n",
        "        for page_num in range(len(doc)):\n",
        "            page = doc.load_page(page_num)\n",
        "            for img in page.get_images(full=True):\n",
        "                xref = img[0]\n",
        "                base_image = doc.extract_image(xref)\n",
        "                image_data = base_image['image']\n",
        "                images.append(image_data)\n",
        "\n",
        "        return images\n",
        "\n",
        "    def analyze_image(self, image):\n",
        "        max_size = (300, 300)\n",
        "        image.thumbnail(max_size)\n",
        "        prompt = \"Given the following image, what can you infer from it?\"\n",
        "        response = self.model.generate_content([image, prompt])\n",
        "        return response.text\n",
        "\n",
        "    def analyze_images_in_parallel(self, images):\n",
        "        descriptions = {}\n",
        "        with ThreadPoolExecutor() as executor:\n",
        "            future_to_image = {\n",
        "                executor.submit(self.analyze_image, Image.open(io.BytesIO(img_data))): idx\n",
        "                for idx, img_data in enumerate(images)\n",
        "            }\n",
        "\n",
        "            for future in as_completed(future_to_image):\n",
        "                idx = future_to_image[future]\n",
        "                try:\n",
        "                    description = future.result()\n",
        "                    descriptions[f\"image_{idx + 1}.jpg\"] = description\n",
        "                except Exception as exc:\n",
        "                    descriptions[f\"image_{idx + 1}.jpg\"] = f\"An error occurred: {exc}\"\n",
        "\n",
        "        return descriptions\n",
        "\n",
        "    def extract_tables(self, pdf_path):\n",
        "        tables = camelot.read_pdf(pdf_path, pages='all')\n",
        "        return tables\n",
        "\n",
        "    def process_paper(self, pdf_path):\n",
        "        with ThreadPoolExecutor() as executor:\n",
        "            # Step 1: Extract and process text\n",
        "            text_future = executor.submit(self.extract_text, pdf_path)\n",
        "\n",
        "            # Step 2: Extract and analyze images\n",
        "            images_future = executor.submit(self.extract_images, pdf_path)\n",
        "\n",
        "            # Step 3: Extract and process tables\n",
        "            tables_future = executor.submit(self.extract_tables, pdf_path)\n",
        "\n",
        "            # Gather results\n",
        "            self.extracted_text = text_future.result()\n",
        "            structured_text, _ = self.process_text(self.extracted_text)\n",
        "\n",
        "            images = images_future.result()\n",
        "            self.image_descriptions = self.analyze_images_in_parallel(images)\n",
        "\n",
        "            tables = tables_future.result()\n",
        "            for i, table in enumerate(tables):\n",
        "                csv_data = table.df.to_csv(index=False)\n",
        "                self.tables_data[f\"table_{i + 1}\"] = csv_data\n",
        "\n",
        "    def handle_query(self, query):\n",
        "        # Concatenate all information\n",
        "        combined_info = f\"{self.extracted_text}\\n\\nImage Descriptions:\\n{self.image_descriptions}\\n\\nTables Data:\\n{self.tables_data}\"\n",
        "\n",
        "        # Generate a response based on the combined information\n",
        "        prompt = f\"Based on the following information, answer the query:\\n\\n{combined_info}\\n\\nQuery: {query}\"\n",
        "        try:\n",
        "            response = self.model.generate_content(prompt)\n",
        "            return response\n",
        "        except Exception as e:\n",
        "            return f\"An error occurred: {str(e)}\"\n",
        "\n",
        "# Example Usage\n",
        "analyzer = ResearchPaperAnalyzer(\"gemini-1.5-pro-latest\")\n",
        "pdf_path = \"/content/Bleph_ipmi__Copy_ (2).pdf\"\n",
        "analyzer.process_paper(pdf_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "0wXHkwwiOZeq",
        "outputId": "989c7132-8311-4767-b94b-0e8ca84b3fa9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: what numbers or accuracy i can use to add in my resume?\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              ">Here are the key numbers and how you can phrase them for your resume, focusing on the strongest points:\n",
              "\n",
              "**For a general audience (non-technical):**\n",
              "\n",
              "* **Developed a system for automatically detecting blepharospasm from videos with 94.4% accuracy.** (This highlights the overall impact and success)\n",
              "* **Built a novel blink detection system achieving over 99% accuracy, outperforming methods using only normal blinks.** (Emphasizes innovation and superior results) \n",
              "\n",
              "**For a technical audience (familiar with ML):**\n",
              "\n",
              "* **Achieved 94.4% accuracy in blepharospasm detection using a novel eye-based feature set and Random Forest classifier.** (Specifics for those who understand the methods)\n",
              "* **Developed a CNN-based blink detector with 99.8% accuracy, including detection of tight blinks often missed by other approaches.** (Showcases deep learning expertise and a solution to a common challenge)\n",
              "* **Improved blepharospasm classification accuracy by [percentage increase, e.g., 15%] over baseline methods using Mediapipe features.** (Demonstrates ability to benchmark and surpass existing solutions)\n",
              "\n",
              "**Additional Tips for Your Resume:**\n",
              "\n",
              "* **Quantify impact:**  Instead of just saying \"improved accuracy,\" state the percentage increase whenever possible.\n",
              "* **Use action verbs:**  Start bullet points with verbs like \"developed,\" \"achieved,\" \"implemented,\" etc.\n",
              "* **Tailor to the job:**  Choose numbers and phrasing that align with the specific skills and experience the job description emphasizes. \n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: exit\n",
            "Bot: Goodbye!\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Initialize the loop\n",
        "while True:\n",
        "    # Ask the user for a query\n",
        "    user_query = input(\"You: \")\n",
        "\n",
        "    # Check if the user wants to exit the chat\n",
        "    if user_query.lower() in [\"stop\", \"exit\", \"quit\"]:\n",
        "        print(\"Bot: Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Handle the query using your analyzer\n",
        "    response = analyzer.handle_query(user_query)\n",
        "\n",
        "\n",
        "    # Display the response\n",
        "    display(Markdown(\">\" + response.text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2Uw4PtR8axL"
      },
      "source": [
        "### Objective\n",
        "- **Designed and Developed** a system to efficiently extract and analyze text, images, and tables from research papers, delivering a comprehensive and scalable solution that overcomes the limitations of current LLM models.\n",
        "\n",
        "### Methods\n",
        "- **Parallel Processing**: **Implemented** parallel processing using `ThreadPoolExecutor` to concurrently extract text, images, and tables from PDFs, **significantly reducing** processing time.\n",
        "- **Image, Text, and Table Analysis**:\n",
        "  - **Image Analysis**: **Extracted** and **analyzed** images using the Gemini model, **generating** detailed descriptions and **storing** them in a dictionary for easy retrieval.\n",
        "  - **Text Extraction**: **Applied** OCR for text extraction and **processed** it using tokenization and POS tagging, **enabling** more accurate interpretation and analysis.\n",
        "  - **Table Extraction**: **Utilized** Camelot to extract and process tables, **organizing** them for structured analysis.\n",
        "\n",
        "### Results\n",
        "- **Enhanced Efficiency, Scalability, and Interactivity**: The system **accelerates** the processing of large research papers and **handles** complex content by integrating text, images, and tables. Additionally, it **incorporates** a chatbot for users to interact with the paper's content, **enhancing** accessibility and understanding.\n",
        "  \n",
        "- **Overcame Token Limitations**: By **processing** text, images, and tables separately, the system **circumvents** the token limitations of current LLM models, **enabling** a more thorough and comprehensive analysis.\n",
        "\n",
        "---\n",
        "\n",
        "The second point is valid, as it correctly highlights how the system's approach to processing different sections independently allows it to avoid the token constraints that typically limit the analysis capabilities of LLMs."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
